{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17dbed11",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnumpy_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m np_config\n\u001b[0;32m      5\u001b[0m np_config\u001b[38;5;241m.\u001b[39menable_numpy_behavior()\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.ops.numpy_ops import np_config\n",
    "np_config.enable_numpy_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d2f0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processing_fasta_file(path, PTM):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    path: string\n",
    "        path to .fasta file with sequences\n",
    "    PTM: list of strings\n",
    "        the phosphorylation site such as ['S', 'T']\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    id_phos_sites: numpy array\n",
    "        array of UniProtIDs and phosphorylation indices\n",
    "    phos_dict : dict\n",
    "        dictionary with Uniprot IDs as keys and phosphorylation sequences \n",
    "    \n",
    "    \"\"\"\n",
    "    fp = open(path, 'r+')\n",
    "    lines = []\n",
    "    with fp as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    ID = []\n",
    "    uniprot_ID = []\n",
    "    c = []\n",
    "    indx = []\n",
    "    p_ind = []\n",
    "    p = []\n",
    "\n",
    "    for i in range(0, len(lines), 2):\n",
    "        ID.append(lines[i])\n",
    "\n",
    "    for i in range(len(ID)):\n",
    "        uniprot_ID.append(ID[i].split(\"|\"))\n",
    "\n",
    "    for lis in range(len(uniprot_ID)):\n",
    "        for i in range(1, len(uniprot_ID[lis]), 2):\n",
    "            c.append(uniprot_ID[lis][i])\n",
    "    phosp = np.array(c)\n",
    "\n",
    "    ID = []\n",
    "    def split(word):\n",
    "        return list(word)\n",
    "\n",
    "    for i in range(1, len(lines), 2):\n",
    "        ID.append([lines[i]])\n",
    "\n",
    "    for lis in range(len(ID)):\n",
    "        for char in range(len(ID[lis])):\n",
    "            indx.append(split(ID[lis][char]))\n",
    "\n",
    "    for lis in range(len(indx)):\n",
    "        p = []\n",
    "        for i in range(len(indx[lis])):\n",
    "            for k in range(len(PTM)):\n",
    "                if indx[lis][i] == PTM[k]:\n",
    "                    p.append(i)\n",
    "        p_ind.append(tuple(p))\n",
    "    \n",
    "    p_ind = np.array(p_ind, dtype=tuple)\n",
    "    if len(p_ind) == 1:\n",
    "        id_phos_sites = np.zeros(shape=(1,2), dtype = object)\n",
    "        id_phos_sites[:,0] = phosp\n",
    "        id_phos_sites[:,1] = tuple(p_ind)\n",
    "    else:\n",
    "        id_phos_sites = np.vstack((phosp, p_ind)).T\n",
    "    phos_dict = dict(np.c_[phosp, ID])\n",
    "    return id_phos_sites, phos_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a7afc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmers(k, phos_dict, PTM):\n",
    "    \"\"\"\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    k : int\n",
    "        an odd number that specifies that length of the kmer with the\n",
    "        phosphorylation site in the middle\n",
    "    phos_dict : dict\n",
    "        dictionary with Uniprot IDs as keys and phosphorylation sequences \n",
    "    PTM: list of strings\n",
    "        the phosphorylation site such as ['S', 'T']\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    kmer : 2D array\n",
    "        2D array with columns UniProt ID, phosphorylation index, k-mer\n",
    "\n",
    "    \"\"\"\n",
    "    kmer = []\n",
    "    ID = list(phos_dict.keys())\n",
    "    for q in tqdm(range(len(ID)), desc=\"Kmers\", position=0, leave=True):\n",
    "        for d in range(len(phos_dict[ID[q]])):\n",
    "            for t in range(len(PTM)):\n",
    "                if phos_dict[ID[q]][d] == PTM[t]:\n",
    "                    around_ind = int((k-1)/2)\n",
    "                    if d-around_ind < 0 and d+around_ind+1 < len(phos_dict[ID[q]]):\n",
    "                        c = d-0\n",
    "                        g = phos_dict[ID[q]][d-c:d+around_ind+1]\n",
    "                        kmer.append([ID[q], d, \"-\"*(around_ind-d) + g])\n",
    "                    elif d-around_ind >= 0 and d+around_ind+1 >= len(phos_dict[ID[q]]):\n",
    "                        c = len(phos_dict[ID[q]]) - d\n",
    "                        g = phos_dict[ID[q]][d-around_ind:d+c]\n",
    "                        kmer.append([ID[q], d, g + \"-\"*(d+around_ind+1-len(phos_dict[ID[q]]))])\n",
    "                    elif d-around_ind < 0 and d+around_ind+1 >= len(phos_dict[ID[q]]):\n",
    "                        c = d-0\n",
    "                        b = len(phos_dict[ID[q]]) - d\n",
    "                        g = phos_dict[ID[q]][d-c:d+b]\n",
    "                        kmer.append([ID[q], d, \"-\"*(around_ind-d) + g + \"-\"*(d+around_ind+1-len(phos_dict[ID[q]]))])\n",
    "                    else:\n",
    "                        kmer.append([ID[q], d, phos_dict[ID[q]][d-around_ind:d+around_ind+1]])\n",
    "    \n",
    "    kmer = np.array(kmer)\n",
    "    return kmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47c9048b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_predict(sequence_path, modification, cutoff):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    sequence_path: string\n",
    "        path to .fasta file with sequences\n",
    "    modification: string\n",
    "        phos, glycosylation, hydroxyproline, CDK, CK2, MAPK, PKA, or PKC \n",
    "    cutoff: float\n",
    "        number between 0 and 1 to add PTM labels to k-mers\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    probability: numpy array\n",
    "        array with PTM indices and probability\n",
    "    \n",
    "    \"\"\"\n",
    "    if modification == \"glycosylation\":\n",
    "        PTM = ['N']\n",
    "    elif modification == \"hydroxyproline\":\n",
    "        PTM = ['P']\n",
    "    else:\n",
    "        PTM = ['S', 'T']\n",
    "    \n",
    "    id_phos_sites, phos_dict = processing_fasta_file(sequence_path, PTM)\n",
    "    kmer = kmers(35, phos_dict, PTM)\n",
    "    # One-hot encoding\n",
    "    alphabet = \"ARNDCEQGHILKMFPSTWYV-U\"\n",
    "\n",
    "    def convert_to_onehot(data):\n",
    "        #Creates a dict, that maps to every char of alphabet an unique int based on position\n",
    "        global char_to_int\n",
    "        char_to_int = dict((c,i) for i,c in enumerate(alphabet))\n",
    "        encoded_data = []\n",
    "        #Replaces every char in data with the mapped int\n",
    "        encoded_data.extend([char_to_int[char] for char in data])\n",
    "        return encoded_data\n",
    "\n",
    "\n",
    "    def tensor_encoding(k, x_data, depth):\n",
    "        indices = []\n",
    "        t2 = []\n",
    "        for i in range(len(x_data)):\n",
    "            indices.append(convert_to_onehot(x_data[i,2]))\n",
    "        for i in range(len(indices)):\n",
    "            t1 = tf.one_hot(indices[i], depth)\n",
    "            t2.append(t1)\n",
    "        return t2\n",
    "\n",
    "    tensor = tensor_encoding(35, kmer, 22)\n",
    "    np.save('dataset', tensor)\n",
    "    dataset = np.load('dataset.npy', allow_pickle=True).astype('float32')\n",
    "    dataset = tf.convert_to_tensor(dataset)\n",
    "    dataset = dataset.reshape(dataset.shape[0], 35, 22, 1)\n",
    "\n",
    "    # Recreate the exact same model, including its weights and the optimizer\n",
    "    new_model = tf.keras.models.load_model(f'{modification}_no_labels.h5')\n",
    "\n",
    "    # Show the model architecture\n",
    "    new_model.summary()\n",
    "\n",
    "    predictions = new_model.predict(dataset)\n",
    "    \n",
    "    if PTM == ['S', 'T']:\n",
    "        for i in range(len(predictions)):\n",
    "            if predictions[i] > cutoff:\n",
    "                if phos_dict[kmer[i,0]][int(kmer[i,1])] == \"S\":\n",
    "                    phos_dict[kmer[i,0]] = phos_dict[kmer[i,0]][0:int(kmer[i,1])]+\"X\"+phos_dict[kmer[i,0]][int(kmer[i,1])+1:]        \n",
    "                elif phos_dict[kmer[i,0]][int(kmer[i,1])] == \"T\":\n",
    "                    phos_dict[kmer[i,0]] = phos_dict[kmer[i,0]][0:int(kmer[i,1])]+\"Z\"+phos_dict[kmer[i,0]][int(kmer[i,1])+1:]        \n",
    "        \n",
    "        kmer = kmers(35, phos_dict, ['S', 'T', 'X', 'Z'])\n",
    "\n",
    "        # One-hot encoding\n",
    "        alphabet = \"ARNDCEQGHILKMFPSTWYVXZ-U\"\n",
    "\n",
    "        tensor = tensor_encoding(35, kmer, 24)\n",
    "        np.save('dataset', tensor)\n",
    "        dataset = np.load('dataset.npy', allow_pickle=True).astype('float32')\n",
    "        dataset = tf.convert_to_tensor(dataset)\n",
    "        dataset = dataset.reshape(dataset.shape[0], 35, 24, 1)\n",
    "    else:\n",
    "        for i in range(len(predictions)):\n",
    "            if predictions[i] > cutoff:\n",
    "                if phos_dict[kmer[i,0]][int(kmer[i,1])] == PTM[0]:\n",
    "                    phos_dict[kmer[i,0]] = phos_dict[kmer[i,0]][0:int(kmer[i,1])]+\"X\"+phos_dict[kmer[i,0]][int(kmer[i,1])+1:]        \n",
    "                \n",
    "        kmer = kmers(35, phos_dict, [PTM[0], 'X'])\n",
    "\n",
    "        # One-hot encoding\n",
    "        alphabet = \"ARNDCEQGHILKMFPSTWYVX-U\"\n",
    "\n",
    "        tensor = tensor_encoding(35, kmer, 23)\n",
    "        np.save('dataset', tensor)\n",
    "        dataset = np.load('dataset.npy', allow_pickle=True).astype('float32')\n",
    "        dataset = tf.convert_to_tensor(dataset)\n",
    "        dataset = dataset.reshape(dataset.shape[0], 35, 23, 1)       \n",
    "    \n",
    "\n",
    "    # Recreate the exact same model, including its weights and the optimizer\n",
    "    new_model = tf.keras.models.load_model(f'{modification}_with_labels.h5')\n",
    "\n",
    "    # Show the model architecture\n",
    "    new_model.summary()\n",
    "\n",
    "    predictions = new_model.predict(dataset)\n",
    "    probability = np.c_[kmer[:,1], predictions]\n",
    "    return probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "29f528bf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Kmers: 100%|█████████████████████████████████████| 1/1 [00:00<00:00, 814.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_39\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_39 (Conv2D)          (None, 33, 20, 16)        160       \n",
      "                                                                 \n",
      " max_pooling2d_39 (MaxPoolin  (None, 16, 10, 16)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_39 (Flatten)        (None, 2560)              0         \n",
      "                                                                 \n",
      " dense_78 (Dense)            (None, 32)                81952     \n",
      "                                                                 \n",
      " dropout_39 (Dropout)        (None, 32)                0         \n",
      "                                                                 \n",
      " dense_79 (Dense)            (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 82,145\n",
      "Trainable params: 82,145\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Kmers: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 1206.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_42\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_42 (Conv2D)          (None, 33, 22, 16)        160       \n",
      "                                                                 \n",
      " max_pooling2d_42 (MaxPoolin  (None, 16, 11, 16)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_42 (Flatten)        (None, 2816)              0         \n",
      "                                                                 \n",
      " dense_84 (Dense)            (None, 32)                90144     \n",
      "                                                                 \n",
      " dropout_42 (Dropout)        (None, 32)                0         \n",
      "                                                                 \n",
      " dense_85 (Dense)            (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 90,337\n",
      "Trainable params: 90,337\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "[['10' '0.87756836']\n",
      " ['21' '0.0468041']\n",
      " ['22' '0.04471481']\n",
      " ['23' '0.066173136']\n",
      " ['24' '0.09452835']\n",
      " ['26' '0.12271857']\n",
      " ['30' '0.11969432']\n",
      " ['35' '0.10305083']\n",
      " ['36' '0.14250287']\n",
      " ['40' '0.4742926']\n",
      " ['45' '0.46295208']\n",
      " ['48' '0.69296926']\n",
      " ['50' '0.8063079']\n",
      " ['54' '0.80108714']\n",
      " ['65' '0.6136423']\n",
      " ['80' '0.7408643']\n",
      " ['83' '0.2005311']\n",
      " ['86' '0.785535']\n",
      " ['93' '0.17783576']\n",
      " ['98' '0.36507028']\n",
      " ['106' '0.023202807']\n",
      " ['110' '0.04163432']\n",
      " ['111' '0.13294938']\n",
      " ['131' '0.20905817']\n",
      " ['137' '0.054080218']\n",
      " ['139' '0.041570514']\n",
      " ['140' '0.08655992']\n",
      " ['145' '0.023040324']\n",
      " ['149' '0.031866014']\n",
      " ['150' '0.014912307']\n",
      " ['151' '0.017421812']\n",
      " ['158' '0.061077744']\n",
      " ['161' '0.18363935']\n",
      " ['163' '0.40533054']\n",
      " ['167' '0.11593455']\n",
      " ['169' '0.45304218']\n",
      " ['177' '0.95078623']\n",
      " ['184' '0.34713125']\n",
      " ['195' '0.54320556']\n",
      " ['199' '0.58353555']\n",
      " ['207' '0.122703165']\n",
      " ['210' '0.053562164']\n",
      " ['212' '0.13021469']\n",
      " ['223' '0.6210822']\n",
      " ['248' '0.9796685']\n",
      " ['253' '0.96933484']\n",
      " ['256' '0.87657607']\n",
      " ['263' '0.8475679']\n",
      " ['273' '0.086360425']\n",
      " ['277' '0.01924628']\n",
      " ['280' '0.11472574']\n",
      " ['282' '0.6233439']\n",
      " ['289' '0.056860745']\n",
      " ['290' '0.124748975']\n",
      " ['291' '0.23815465']\n",
      " ['295' '0.105352044']\n",
      " ['297' '0.7643502']\n",
      " ['310' '0.9743805']\n",
      " ['311' '0.9648514']\n",
      " ['320' '0.33039707']\n",
      " ['324' '0.18524474']\n",
      " ['334' '0.047716796']\n",
      " ['336' '0.08398196']\n",
      " ['337' '0.1785796']\n",
      " ['347' '0.18531236']\n",
      " ['366' '0.06870839']\n",
      " ['369' '0.034089804']\n",
      " ['371' '0.066511184']\n",
      " ['372' '0.14247265']\n",
      " ['394' '0.21506083']\n",
      " ['398' '0.31671008']\n",
      " ['409' '0.6559156']\n",
      " ['410' '0.76035506']\n",
      " ['415' '0.82548237']\n",
      " ['416' '0.79179645']\n",
      " ['418' '0.74542236']\n",
      " ['438' '0.26729715']\n",
      " ['445' '0.19462276']\n",
      " ['449' '0.06801322']\n",
      " ['450' '0.09185386']\n",
      " ['462' '0.12941834']\n",
      " ['466' '0.088676274']\n",
      " ['503' '0.06884909']\n",
      " ['512' '0.110342056']\n",
      " ['513' '0.2741536']\n",
      " ['517' '0.019139916']\n",
      " ['519' '0.02414462']\n",
      " ['520' '0.033449948']\n",
      " ['522' '0.041914135']\n",
      " ['523' '0.020730793']\n",
      " ['524' '0.01466468']\n",
      " ['526' '0.035020143']\n",
      " ['527' '0.08480927']\n",
      " ['528' '0.20924562']\n",
      " ['532' '0.14653537']\n",
      " ['535' '0.09478259']\n",
      " ['538' '0.14407375']\n",
      " ['540' '0.39995766']\n",
      " ['541' '0.5878965']\n",
      " ['543' '0.75081956']\n",
      " ['547' '0.28395456']\n",
      " ['548' '0.15600511']\n",
      " ['550' '0.18622881']\n",
      " ['560' '0.23926872']\n",
      " ['561' '0.5020997']\n",
      " ['569' '0.50626904']\n",
      " ['583' '0.9636034']\n",
      " ['606' '0.6761851']\n",
      " ['618' '0.30712277']\n",
      " ['630' '0.28974396']\n",
      " ['647' '0.3452258']\n",
      " ['666' '0.13827953']\n",
      " ['671' '0.06674141']\n",
      " ['672' '0.07302594']\n",
      " ['673' '0.1025911']\n",
      " ['680' '0.21577835']\n",
      " ['696' '0.23292762']\n",
      " ['709' '0.62416506']\n",
      " ['723' '0.993171']\n",
      " ['725' '0.9919777']\n",
      " ['728' '0.9742805']\n",
      " ['733' '0.8811016']\n",
      " ['743' '0.1419949']\n",
      " ['751' '0.084528595']\n",
      " ['753' '0.38889816']\n",
      " ['758' '0.49160168']\n",
      " ['768' '0.039645135']\n",
      " ['771' '0.26405635']\n",
      " ['777' '0.08070859']\n",
      " ['790' '0.1890648']\n",
      " ['821' '0.9599378']\n",
      " ['833' '0.27503055']\n",
      " ['845' '0.2796265']\n",
      " ['852' '0.23904863']\n",
      " ['878' '0.9519504']\n",
      " ['883' '0.98959816']\n",
      " ['890' '0.9909126']\n",
      " ['895' '0.9899466']\n",
      " ['920' '0.07826015']\n",
      " ['923' '0.54034895']\n",
      " ['933' '0.27468374']\n",
      " ['934' '0.21624619']\n",
      " ['943' '0.95430374']\n",
      " ['951' '0.3697826']\n",
      " ['972' '0.6140626']]\n"
     ]
    }
   ],
   "source": [
    "print(sequence_predict('/Users/aliakassim/Documents/Ire1a.fasta', \"PKC\", 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2c4328",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
